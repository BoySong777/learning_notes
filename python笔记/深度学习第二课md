### 第一周

- 主要内容：超参数调优，如何构建数据，如何确保优化算法快速运行
- 应用深度学习是一个典型的迭代过程，很难一次就达到完美效果，需要多次 idea ---->code ------>experiment循环往复，循环该过程的效率是决定项目进展速度的一个关键因素
- 在机器学习中通常将样本分为训练集（训练模型）、验证集（验证和调整模型）、测试集（评估模型）三部分，要确保验证集合测试集来自同一个分布

- 偏差（Bias）：描述的是预测值的期望与真实值之间的差距，偏差越大，越偏离真实数据
- 方差（Variance）：描述的是预测值的变化范围，离散程度，方差越大，数据越分散。

- 偏差高 称为 欠拟合，方差高称为过度拟合
- 交叉验证：简单来说就是重复使用数据。除去测试集，把剩余数据进行划分，组合成不同的训练街和验证集，某次在训练集中出现的样本下次可能成为验证集中的样本，这就是所谓的“交叉”
- 一般来说最优误差也被称为基本误差

- 解决偏差和方差过高的方法（以下方法不一定有效，也仅限于这些方法，关键的一点就是多尝试）

  <img src="E:\TyporaResources\Image\image-20210924104638570.png" alt="image-20210924104638570" style="zoom:50%;" />

- 解决高方差的其中两个相对有效的方法：1、正则化，2、准备更多的数据

- 向量范数：一个向量的n范数就是该向量各个维度值n次方后在开n次方。参考文档：https://www.jianshu.com/p/f0e41ebe5e4b

- L<sub>2</sub> 正则化：
  $$
  J(w,b) =\frac{1}{m}\sum_{i=1}^m \zeta(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_{2}^{2}
  $$

$$
||w||_{2}^{2} =\sum_{j=1}^{n_x} w_j^2 = w^Tw
$$

​			$\lambda$  称为正则化参数，它属于超参数，$||w||_2^2$ 称为w范数的平方，L<sub>2</sub>正则化化就是把成本函数（logistic回归函数）加上w范数的平方

​		ps：在python中 lambda 是一个保留字段，所以通常用 lambd字段 表示	$\lambda$  （去掉了a）

- L<sub>1</sub> 正则化：
  $$
  J(w,b) =\frac{1}{m}\sum_{i=1}^m \zeta(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}||w||_{1}
  $$

- 人们在训练网络时，倾向于使用L<sub>2</sub>正则化

- 在多层神经网络中，把成本函数向量化：
  $$
  J(W,b) =\frac{1}{m}\sum_{i=1}^m \zeta(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^{L}||w^{[l]}||_{F}^{2}
  $$

  $$
  ||w^{[l]}||_{F}^{2} =\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}} (w_{ij})^2 　　　ｗ：（n^{[l]}，n^{[l-1]})
  $$

  

- $||w^{[l]}||_{F}^{2}$ 是矩阵范数，叫做Frobenius（弗罗贝尼乌斯范数）

- 如何使用带有该矩阵范数的成本函数实现梯度下降：

  dw会发生变化   $dW^{[l]} = dz^{[l]}{a^{[l-1]}}^{T}$  ====>>  $dW^{[l]} = dz^{[l]}{a^{[l-1]}}^{T} + \frac{\lambda}{m}W^{[l]}$ 

  所以 $W^{[l]} = W^{[l]} - \alpha dW^{[l]} = (1-\alpha*\frac{\lambda}{m})W^{[l]}-\alpha*dz^{[l]}{a^{[l-1]}}^{T}$

  $W^{[l]} $肯定是越来越小，所以L<sub>2</sub>正则化也被称为“权重衰减”

- 正则化为什么可以预防过度拟合？

  直观上来说，如果$\lambda$设置的足够大，则W就可以接近于0，也就是参数权重接近于零，那么这样会导致隐藏单元对整个神经网络的影响就非常小，几乎等于没有隐藏单元，这样神经网络的规模就相当于于一个规模很小的网络，这就会使神经网络变成高偏差状态，也就是欠拟合状态。当然这只是一个极端的状态，实际不会出现，$\lambda$的值肯定存在一个合适的值，使神经网络从过拟合状态变成刚刚好的一个状态。

  <img src="E:\TyporaResources\Image\image-20210924151850955.png" alt="image-20210924151850955" style="zoom:67%;" />

从激活函数方面讲，在神经网络中如果每层激活函数都是线性的，那么整个网络就是一个线性网络，既使这个网络非常深。

 	<img src="E:\TyporaResources\Image\image-20210924153013374.png" alt="image-20210924153013374" style="zoom:50%;" />

![image-20210924153049763](E:\TyporaResources\Image\image-20210924153049763.png)

- dropout（随机失活）正则化：个人理解，就是对神经网络中的节点设置失效的概率，一个节点可能会被剔除，把一些节点剔除后，整个神经网络就会变得相对简单，则不容易出现过度拟合的情况

- 如何实施dropout呢？
  - inverted dropout（反向随机失活）

    1. 首先设置keep-prop参数，确定让隐藏节点失活的概率，例如keep-prop = 0.8，那么节点失活的概率就为0.2。

    2. 为每一层创建d，例如第3层就是d3 = np.random.rand(a3.shape[0],a3.shape[1]);  d 和 a 的维度相同，

    3. d中每一个元素与keep-prop比大小： d3 = d3<keep-prop,  此时d3 中的元素是True和False，

    4.  更新a的值：a3 *= d3,  此时a3 中有些元素没有变，有些元素变成了0， 这就相当于把一些节点失活。失活的概率也是20%

    5. 然后再次更新a ： a3 /= keep-prop   这一步的目的是为了不影响z的值，因为$z^{[4]} = w^{[4]}a^{[3]}+b^{[4]}$, a3发生变化，则z4也会跟着变，所以为了不让z4有那么大的波动，我们把a3变化的那么一部分给他找过来，于是就有了 a3 /= keep-prop

    6. 只需训练时正向和反向传播和反向传播时用到以上步骤，到测试阶段就不需要以上步骤

       ps: 不同层的keep-prop可以不同，对于可能出现过拟合且含有诸多参数的层，我们可以把keep-prop设置比较小的值，以便产生更好的dropout。

       ​		从技术上讲虽然也可以用dropout删除几个输入特征，但是在现实中通常不会这儿做，一般把输入层的keep-prop值设为1.

       ​	总结: 如果担心某些层比其它层更容易产生过度拟合，那么就把该层的keep-prop的值调小一些。dropout在计算机视觉方面用的比较多一些。最后，dropout是一种正则化方法，有助于预防过拟合，除非算法过拟合，否则不要使用dropout

    - dropout的缺点：代价函数 J 不能被明确定义， 因为每次迭代都会随机失去一些节点

- 增加数据集的方法：除了传统的增加样本的数量，例如在识别猫的照片中，我们不容易获得很多猫的照片，但是我们可以通过把现有照片翻转、裁剪从而生成假训练数据，和全新的、独立的照片相比这些假数据无法提供那么多信息但是获取代价比较低。

- early stopping：一种预防过拟合的方法。

  ​	提早停止训练神经网络

  <img src="E:\TyporaResources\Image\image-20210926214328849.png" alt="image-20210926214328849" style="zoom: 25%;" />

- L<sub>2</sub>正则化的缺点：找到合适的$\lambda$比较耗时；early shopping的缺点：不能兼顾损失函数J 的最小化和防止过度拟合两方面。

- 归一化（normalization）: 在神经网络训练时，一种加速训练的方法。使用场景：当训练样本的不同特征值的范围不同时，比如x1的范围是[0,1]，而x2的范围是[0,1000], x1和x2的范围差距过大，这时使用归一化来处理输入特征值后再训练，训练的用时会减少，效率会升高。

- 为什么要使用归一化：如果样本中不同的输入特征值的范围差距过大，则会导致成本函数 J  在坐标轴上呈现的是一个非常 扁长的图形，在梯度下降的过程会非常曲折，非常耗时。而归一化输入特征值后，不同的输入特征值的范围就类似了，成本函数 J 的图形是相对较圆滑的，进而在梯度下降时会相对平滑，很快就会找到最低点。

  ![image-20210927151304399](E:\TyporaResources\Image\image-20210927151304399.png)

- 归一化的步骤：

  ​	步骤一：零均值化。目的是把所有样本都移动到原点周围  ($\mu$是一个向量，表示所有样本的平均值)
  $$
  \mu = \frac{1}{m}\sum_{i=1}^{m}X^{(i)}\\
  X = X - \mu
  $$
  

<img src="E:\TyporaResources\Image\image-20210927145239879.png" alt="image-20210927145239879" style="zoom:50%;" />

  		步骤二：归一化方差（normalize the variances）。目的是要让每一个特征值的方差都等于1。($\sigma^2$是一个向量，代表X方差的平均值)
$$
\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}{X^{(i)}}^2\\
X = X/\sigma^2
$$
<img src="E:\TyporaResources\Image\image-20210927150257053.png" alt="image-20210927150257053" style="zoom:50%;" />

- <font color='red'>提示：如果在训练神经网络时使用了归一化，那么在测试数据时也要使用与训练时相等的$\mu和\sigma^2$来处理测试样本，而不是使用测试样本重新得到一个新的$\mu和\sigma^2$</font>

- 如果输入特征值处于不同的范围内，那么归一化特征值就非常重要了，相反，如果特征值处于相似范围内，那么归一化就不是很重要了，执行归一化不会产生什么危害，可以经常对特征值做归一化处理，不管你能不能确定它真的能提高速度。

- 提升神经网络训练速度的方法。

- 梯度消失和梯度爆炸：当W中的元素与1相比过大时容易发生梯度爆炸，比1相比过小时容易发生梯度消失。

- 为W选择一个合适的初始值有助于预防梯度消失和梯度爆炸，那如何设置W的初始值呢，现在有一个相对不正式但是有效的方案，该方案有助于我们更好地选择随机初始化参数来防止梯度消失或梯度爆炸：

  ​			假设现在要初始化第l层的W，第l-1层有n个输出，首先设置一个变量 var_w = $\frac{1}{n}$，

  ​			$W^{[l]}$ = np.random.randn(.....) * np.sqrt(var_w)

  ​	这样初始化W可以相对有效预防梯度消失和梯度爆炸，经验表明，当该层神经网络使用的激活函数是ReLU函数时，var_w = $\frac{2}{n}$效果会更好，而当激活函数是tanh时，var_w = $\frac{1}{n}$效果会更好，这没有一个明确的规定，这是给了一个相对合理的默认值，在训练的时候可以做出相应的调整，这也算是一个超参数，但是根据经验表明，虽然调整该超参数也有一定的效果，但是相比其他超参数，调优这个超参数的效果一般，通常把它的优先级放的较低。

- 对计算梯度做数值逼近:  在求一点的导数时，使用双边误差的方法比使用单边误差更逼近导数值，可以使用双边误差来判断别人给你的函数g($\theta$)是否正确实现了函数 f 的导数，

  ![image-20210927173400655](E:\TyporaResources\Image\image-20210927173400655.png)

- 梯度检验(grad check)：一种用于检验在backprop过程中是否存在bug的方法，它的主要方法就是对计算梯度做数值逼近

  - 具体步骤：

    - 前提：神经网络中有很多层，每一层都有对应的W和b

    - 把每一层的W(矩阵) 转换成一个向量，b本来就是向量，然后把这些向量（W和b）做连接，形成一个巨形的向量$\theta$

    - 代价函函数J（W,b）变成了J($\theta$) ,

    - 然后把梯度下降得来的dW，db 以同样的方式转换成$d\theta$

      <img src="E:\TyporaResources\Image\image-20210927182304264.png" alt="image-20210927182304264" style="zoom: 50%;" />

    - 从整体上看，现在代价函数已经变成$J(\theta) = J(\theta_1,\theta_2,\theta_3,...,\theta_i,...)$

    - 开始实施梯度检验，我们需要对一下步骤循环执行

    - 使用双边误差梯度检验求出大致的 $d\theta[i]$, 记为：$d\theta_{approx}[i] = \frac{J(\theta_1,\theta_2,\theta_3,...,\theta_i+\epsilon\,...)-J(\theta_1,\theta_2,\theta_3,...,\theta_i-\epsilon,...)}{2\epsilon}$;                    approx: 大约

    - 然后判断$d\theta_{approx}[i]$是否约等于$d\theta[i]$ ，判断方法如下：
      $$
      error[i] = \frac{||d\theta_{approx}[i]-d\theta[i]||_2}{||d\theta_{approx}[i]||_2+||d\theta[i]||_2}
      $$
      error[i]是误差，分子是$d\theta_{approx}[i]-d\theta[i]$的欧几里得范数，注意这里没有平方，这是一个欧式距离，分母的作用只是在预防这些向量太大或太小。

      在实际操作中可能$\epsilon$ = $10^{-7}$, 如果error[i]约等于或者小于$\epsilon$, 则一般都是正常的状态，否则就可能有问题，特别是相差很大时，大概率就是有bug了。

- 在神经网络中实施梯度检查的技巧和注意事项：

  - 不要在训练中使用梯度检查，它只用于调试。计算出所有的$d\theta_{approx}[i] $是一个漫长的过程。
  - 如果如果error[i]远大于$\epsilon$,我们要做的是找到对应的i值，定位到相应的层，看看是哪个导致的
  - 如果使用正则化，则需要注意J的正则项
  - 梯度检查不能和dropout同时使用，如果非要使用，请把dropout中的keep_prop设置为1

